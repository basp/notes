# Disclaimer
This is just a huge brain dump to get my thoughts down.

# Summary
For the servo performance ACPDs we need to make use of signal tracing under the constraint that we cannot stop and restart tracing while executing the measurement sequence under most circumstances. Unfortunately, the circumstance where we *are* allowed to stop and start tracing (on repeats) does not help us much in solving the general issue of correlating samples from the raw trace data with *metadata* values such as the position (care area), die, repeat and scan direction. 

This document explains the context and also discusses the problem in more detail before offering a few tentative solutions.

# Scope
This document will only consider the following cases:

* SLP (sparse leap scan)
* DLP (dense leap scan)
* SCP (sparse continous scan)
* DCP (dense continious scan)

The BF case will be included at a later date and perhaps in a seperate document.

# Problem
It's not possible to add custom *metadata* (attributes) to the raw trace signals since these are eventually generated by hardware. In order to create the final performance reports we need to calculate over the trace data in various dimensions. However, since the trace data is bare time series data (timestamp and value) we are left with the problem of how to select the right subsets (according to the requested diimensions) from the bare time series data in order to produce our reports.

As an example, consider the code:
```
struct Sample
{
    public TimeSpan Timestamp;
    public double Value;
}
```

And some (simplified) trace result:
```
IEnumerable<Sample> ssErrorX = ...
```

That result has `Sample` values from the whole measurement sequence. One of the questions we need to answer in the reports is "what was the peak error for the first die" or "what was the peak error for the 5th care area". With the raw trace results we simply cannot answer those questions so we need to find a way to do so.

# Things we know
This section lists an overview of the things we do know and/or can deduct from information we already have.

## The measurement sequence
On a simplified level, the measurement sequence consists of moving, scanning or moving and scanning at the same time. Moving and alternating between moving and then not moving and scanning is known as leap scan (LS). Scanning while moving (at a constant velocity) is known as continous scan (CS).

A simplified LS sequence looks like this:

1. Move to position (care area)
2. Scan for some acquisition time
3. Repeat

A simplified CS sequence looks like this:
1. Move to position (care area)
2. Move and scan for some duration
3. Repeat

Whatever the case and even if the measurement sequence is more complicated, in theory it should be possible to have the measurer (or whatever component is actually executing the sequence) construct a history of things it did. The sequence generating algorithm should behave in a predictable fashion so we could use this data.

## SPG
The *setpoint phase generator* signal *should* give us some extra insight in what setpoint phase a subset of samples belongs to. However this assumes that trace points are traced synchronously. In other words, sample `SSErrorX[i]` and `SPG[i]` belong to the same sample point where `time(SSErrorX[i]) = time(SPG[i])` within some epsilon of equality.

* It seems the SPG signal does not exactly map to the terms used in the EDS to describe the motion profiles. More investigation is needed to find out whether we can map the two or find some useful commmon ground.
* It also remains to be seen whether the assumption of time mentioned in the description above holds true.

## Measurer timestamps
Since we *know* the measurement sequence, we could have the component executing it add timestamps to its action history. If the timestamps are *accurate enough* it would be a very simple way of selecting the right ranges from the trace results. 

* If there's a way to receive events from hardware when certain actions are executed and completed that would be fabulous.
* Since actions are probably queued in some fashion there's no guarantee that time of queuing matches (reasonably) with time of execution. (Can we compensate for this or gain better timestamps in software?)

# Tentative solutions
Some tentative ideas already presented themselves in the previous section. 

## Smart signal processing
If we can *parse* the signal stream (or sequence of samples) with a set of assumptions corresponding to LS, CS and dense and sparse movements, we *might* be able to recreate the measurement sequence only by looking at the raw samples.

Assuming we are able to *tokenize* this data into meaningful phases (i.e. tokens in the parsing vocabulaire) such as, for example, `Idle`, `Scan`, `Move`, `MoveScan` (with samples) then it would already be a lot easier to associate this with the sequence performed by the measurer.

Simplified it would look like this:
```
class RawTraceData
{
    public IEnumerable<Sample> SSErrorX { get; }
    public IEnumerable<Sample> SSErrorY { get; }
    // ...
}

// Just an example, depends on how smart we can get.
enum SetpointPhase
{
    Idle,
    Move,
    Scan,
    MoveScan,
    // ...
}

class SetpointPhaseToken
{
    public SetpointPhase SetpointPhase { get; }
    public RawTraceData { get; } // contains only samples for this phase
}

interface ISampleTokenizer
{
    IEnumerable<SetpointPhaseToken> Tokenize(RawTraceData data);
}
```

We would then use a parser or similar component to make sense of the stream of tokens and parse it into something more useful. In our case it would probably do some transformations and feed it further down the pipeline and eventually associate it with the desired metadata such as die, position (care area), repeat and scan direction for example.

* We could combine tokenizing or parsing if we do not mind creating a bit bulkier component(s).
* It is possible to do some kind of validation or sanity check with the *things we know* and try to see if the thing we parsed even makes sense with regards to the measurement sequence. For example, we can compare expected values with values from the parse and perform some heuristics (see also guided parsing below).
* This might become really involved depending on how much assumptions we can make about the measurement sequence.
* When we want to *parse* the trace data we already know the measurement sequence that we executed. Maybe we can use this as some kind of *guidance* for the parsing algorithm?
* There is probably more ideomatic names to be found already in the code base.

## Simple timestamps
Since the measurer (or some component) is eventually just sending basic actions to some other (more low level service) it could just do the simple thing and add its own timestamp to every action that it records in order to keep a history. If this turns out to be a viable solution it will be straightforward to parse the raw data.

* It's really doubtful whether this will be accurate enough or if we can compensate for the inaccuracies with software.

## Guided parsing
Instead of trying to parse the signal as best as we can and only *then* trying to correlate it to the original movement sequence we can also use the movement sequence as guidance in order to parse the signal stream. This is possible since we need to store the full sequence trace anyway so we only (reasonably) parse it at the end.

* It would be very nice to be able to parse the samples on the fly.
* Maybe we can even involve simple timestamps into the guidance.
* From the measurement sequence we can basically reconstruct the expected values so we can use all kind of heuristics in order to correlate and validate sampled values to their expected values. Needs more investigation though.

## Basic signal processing
Tentatively, it seems possible to identify the phases of interrest in the huge stream of samples.

Assuming the initial constant velocity is trimmed then we can recognize the following scenarios:

* Setpoint velocity is constant within some *epsilon* of zero (LS)
* Setpoint velocity is constant within some *epsilon* of velocity (not zero, CS)

This *should* be enough in order to correlate sample ranges with the measurement sequence as long as measurers and signal parsers agree on what constitutes a *scan*.

* This assumes the settle time is a *known thing*.
* This assumes no extraneous moves (i.e. every move is followed by a scan for LS cases and every move at continuous speed is a scan for CS cases).
* This might work a lot better for LS cases where it appears to be a lot easier to filter the *leaps* from the *scans*.
* It is unclear whether setpoint velocity is actually available as a traced value. Assuming it is not, we might have to use different algorithms to determine state or calculate velocity ourselves by differentating on the position delta.
* Alternatively, we might also be able to calculate velocity by integrating on the acceleration data if that is available.

# Notes
* We need to be able to identify *settle time*. However, this seems to be a known time and the naive approach (looking at velocity) also includes settle time. This way, we can just obtain the settle samples from the total "scan" samples.
